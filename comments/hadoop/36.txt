As if we're not split enough on this decision, let me throw in one more option.   The final and correct answer is that users should set up distributed cache deployment (per https://hadoop.apache.org/docs/r3.0.0-alpha1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html). At that point things just work, and upgrades become simpler. The problem is that distributed cache deployment isn't an option for out of the box.   Spark does not have the problem we're trying to solve here. Every time you submit a Spark application, they ship the assembly JAR via the distributed cache. If we want to push users to distributed cache deployment, maybe the way to solve the out-of-box problem is the light version of dist cache deployment: -libjars.   Option #5) Add a MapReduce property that controls whether the MapReduce JARs are automatically shipped with the job via -libjars, and turn it on by default. Yes, it's inefficient (in both time and space), but it works out of the box and is a natural segue into dist cache deployment.