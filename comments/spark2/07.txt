Thanks for this design doc Aaron.    It looks good for the first implementation to support composite objectives, A&amp;T updates, but I'm not sure about backtracking.   Have you thought about how many passes through the data backtracking can require? As you mention: per backtracking inner loop iteration, we need 2 shuffles. But how many iterations of the inner backtracking loop can be typical? Could it be better in a distributed environment to avoid backtracking and use a constant step size? Especially for the well-behaved objectives we have (e.g. logistic regression). If a constant step size works fast enough, we should do that first - what do you think?   Please try a constant step size first - if it works, that will bring down the communication cost greatly.   It’s fine that the initial implementation will not include the linear operator optimizations present in TFOCS. That’s a good call. In general let’s try to keep the first PR as simple as possible.   Please make sure your code adheres to this example for LBFGS, so we can swap out the Optimizer with your contribution:  http://spark.apache.org/docs/latest/mllib-optimization.html#l-bfgs